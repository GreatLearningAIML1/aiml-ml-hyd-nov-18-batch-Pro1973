{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_Hyd_Nov18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyfMmMnPJjvn"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjcGOJhcJjvp"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR0Pl2XjJjvq"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qr75v_UYJjvs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "635167e3-c26c-41cb-fbed-6faaf8897481"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "#from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTI42-0qJjvw"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2sf67VoJjvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e8195367-e3e4-4f15-a68e-65bddc73cae2"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjiVuxcHkKRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f1d14a0-497f-4538-f463-d32adf02fc1c"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zewyDcBlJjv1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WytT2eRnJjv4"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XycQGBSGJjv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f34f6b51-f517-45ae-c94a-030f918c1200"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK_7d8IUlsvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbe07a40-637c-45f0-a071-9f13188a73d9"
      },
      "source": [
        "x_train[0].dtype"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sD1zrYql_fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh1zLfkZmyEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fecf3f68-0be2-4997-8118-ae51e31d1607"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.value_counts(y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    6000\n",
              "8    6000\n",
              "7    6000\n",
              "6    6000\n",
              "5    6000\n",
              "4    6000\n",
              "3    6000\n",
              "2    6000\n",
              "1    6000\n",
              "0    6000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jtdZ7RqJjv8"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWncbncn8PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAD3q5I6Jjv9",
        "colab": {}
      },
      "source": [
        "# pd.value_counts(y_train)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgHSCXy3JjwA",
        "colab": {}
      },
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xO5BRBzBJjwD"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3fUQpMHxJjwE",
        "colab": {}
      },
      "source": [
        "# Normalize the data\n",
        "# x_n = tf.nn.l2_normalize(x,1)\n",
        "x_train = x_train / 255\n",
        "x_test = x_test/ 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Okwo_SB5JjwI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "da5-DwgrJjwM"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPGVQ-JJJjwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58d800ff-b354-4dd1-e89e-00fc15dde16e"
      },
      "source": [
        "x_train = x_train.reshape(60000, 28, 28, 1)\n",
        "x_train.shape\n",
        "\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "x_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFRRTJq8JjwQ"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dWTZYnKSJjwR",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "# from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7tU9JAy2WC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1b168c98-55a9-4b8b-bec0-aba9f003be0c"
      },
      "source": [
        "print('--- THE DATA ---')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- THE DATA ---\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3tai1_M1VTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b4fda39c-dc5f-4686-d357-6715c2db2734"
      },
      "source": [
        "print('--- THE Target DATA ---')\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- THE Target DATA ---\n",
            "y_train shape: (60000, 10)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C18AoS7eJjwU"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DORCLgSwJjwV",
        "colab": {}
      },
      "source": [
        "# Build the Graph\n",
        "# Initialize Sequential model\n",
        "# model = tf.keras.models.Sequential()\n",
        "\n",
        "TRAIN = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCH = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7NS_Qkq3QgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "cb3f6e39-8b24-46df-e104-dac318d3ef12"
      },
      "source": [
        "    # Define model\n",
        "    model2conv = Sequential()\n",
        "\n",
        "    # 1st Conv Layer\n",
        "    model2conv.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model2conv.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model2conv.add(Convolution2D(32, 3, 3))\n",
        "    model2conv.add(Activation('relu'))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model2conv.add(Flatten())\n",
        "    model2conv.add(Dense(128))\n",
        "    model2conv.add(Activation('relu'))\n",
        "\n",
        "    # Prediction Layer\n",
        "    model2conv.add(Dense(10))\n",
        "    model2conv.add(Activation('softmax'))\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    model2conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Store Training Results\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n",
        "\n",
        "    # Train the model2conv\n",
        "    model2conv.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCH, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 23s 381us/step - loss: 0.3683 - acc: 0.8663 - val_loss: 0.2782 - val_acc: 0.8996\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.2257 - acc: 0.9171 - val_loss: 0.2774 - val_acc: 0.8996\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.1637 - acc: 0.9383 - val_loss: 0.2343 - val_acc: 0.9168\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.1119 - acc: 0.9587 - val_loss: 0.2742 - val_acc: 0.9138\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.0761 - acc: 0.9725 - val_loss: 0.3112 - val_acc: 0.9129\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0510 - acc: 0.9818 - val_loss: 0.3454 - val_acc: 0.9145\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0367 - acc: 0.9865 - val_loss: 0.4176 - val_acc: 0.9143\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0252 - acc: 0.9912 - val_loss: 0.4789 - val_acc: 0.9077\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7facbf302e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ju69vKdIJjwX"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2hAP94vJjwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c6bfacc3-808c-4f63-b6ff-5e6041fac2da"
      },
      "source": [
        " # Define model\n",
        "    # Add 2X2 MaxPooling & DropOut\n",
        "    model2conv = Sequential()\n",
        "\n",
        "    # 1st Conv Layer\n",
        "    model2conv.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model2conv.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model2conv.add(Convolution2D(32, 3, 3))\n",
        "    model2conv.add(Activation('relu'))\n",
        "    \n",
        "    # Add Max Pooling here\n",
        "    model2conv.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # Add Dropout here\n",
        "    model2conv.add(Dropout(0.25))\n",
        "\n",
        "    \n",
        "    \n",
        "    # Fully Connected Layer\n",
        "    model2conv.add(Flatten())\n",
        "    model2conv.add(Dense(128))\n",
        "    model2conv.add(Activation('relu'))\n",
        "\n",
        "    # Prediction Layer\n",
        "    model2conv.add(Dense(10))\n",
        "    model2conv.add(Activation('softmax'))\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    model2conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Store Training Results\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n",
        "\n",
        "    # Train the model2conv\n",
        "    model2conv.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCH, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.3914 - acc: 0.8599 - val_loss: 0.2955 - val_acc: 0.8936\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 17s 282us/step - loss: 0.2582 - acc: 0.9050 - val_loss: 0.2451 - val_acc: 0.9114\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 17s 282us/step - loss: 0.2114 - acc: 0.9215 - val_loss: 0.2330 - val_acc: 0.9151\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 17s 282us/step - loss: 0.1796 - acc: 0.9328 - val_loss: 0.2459 - val_acc: 0.9098\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.1503 - acc: 0.9437 - val_loss: 0.2399 - val_acc: 0.9127\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.1309 - acc: 0.9501 - val_loss: 0.2339 - val_acc: 0.9205\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.1089 - acc: 0.9585 - val_loss: 0.2539 - val_acc: 0.9209\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0949 - acc: 0.9637 - val_loss: 0.2511 - val_acc: 0.9222\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0818 - acc: 0.9685 - val_loss: 0.2653 - val_acc: 0.9221\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0708 - acc: 0.9738 - val_loss: 0.2839 - val_acc: 0.9227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7facb6d6ba90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lGTA3bfEJjwa"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F6gX8n5SJjwb"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cbz4uHBuJjwc",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl-8dOo7Jjwf"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpI1_McYJjwg",
        "outputId": "d9ec9378-5eb5-4694-e186-4c044f79c64d",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAACxCAYAAABAxMXKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQB3pxJkiRJUgV4cyZJkiRJFfB/RKVkRT/syf4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmPl5yE8Jjwm"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44ZnDdJYJjwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "892a2222-a172-43ff-a13a-a22b2a2f371e"
      },
      "source": [
        " \n",
        "model2conv.fit_generator(\n",
        "        datagen.flow(x_train, y_train, batch_size = BATCH_SIZE),\n",
        "        samples_per_epoch = x_train.shape[0],\n",
        "        nb_epoch = EPOCH,\n",
        "        validation_data = (x_test, y_test),\n",
        "        callbacks = callback_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  11/1875 [..............................] - ETA: 23s - loss: 3.3203 - acc: 0.3864"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1875, epochs=10)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.7045 - acc: 0.7426 - val_loss: 0.3496 - val_acc: 0.8800\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.5301 - acc: 0.8034 - val_loss: 0.3284 - val_acc: 0.8885\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.4790 - acc: 0.8223 - val_loss: 0.3348 - val_acc: 0.8811\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.4538 - acc: 0.8316 - val_loss: 0.3181 - val_acc: 0.8927\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.4314 - acc: 0.8417 - val_loss: 0.3523 - val_acc: 0.8822\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.4112 - acc: 0.8489 - val_loss: 0.3026 - val_acc: 0.8927\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.4019 - acc: 0.8508 - val_loss: 0.3213 - val_acc: 0.8914\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3949 - acc: 0.8543 - val_loss: 0.3202 - val_acc: 0.8908\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3823 - acc: 0.8586 - val_loss: 0.3511 - val_acc: 0.8791\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7facb46e8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwQQW5iOJjwq"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c1SrtBEPJjwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7df4dd0-29ac-4682-c85a-262d5b8c870e"
      },
      "source": [
        "model2conv.evaluate(x_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 5s 86us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28363443559010826, 0.89535]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBwVWNQC2qZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d13fc1c8-f5ba-4019-b9b6-cfd98dfba1f0"
      },
      "source": [
        "model2conv.evaluate(x_test, y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 87us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35113415158987044, 0.8791]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}